# Fairness-Testing-of-Compressed-Models
Artificial intelligence models are computationally intensive, and deploying them on resource-poor devices is an issue. Various model compression techniques have been introduced to overcome this, and quantization has ben reported as the most unbiased method. Quantization converts the weights and activations into lower-bit form, significantly reducing the size and complexity of the model, but at the cost of a slight loss in predictive performance. The impact of compression on fairness in natural language processing, computer vision, and large language models has been thoroughly researched, and analysis of classification tasks on tabular data has been relatively less explored. This paper examines the influence of compression via dynamic post-training quantization and quantization-aware training on predictive accuracy, as well as fairness of neural networks on tabular data. Evaluation is done on the Adult, COMPAS, and Student datasets, where predictive performance is assessed by F1 Score and fairness by Statistical Parity Difference, Average Odds Difference, and Disparate Impact.
